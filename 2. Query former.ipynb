{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d0f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import operator\n",
    "from langgraph.graph import END\n",
    "import os\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ed6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://expazure-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0f5cb3c2ad3d40d9a76142c3f49cad9f\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt35\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d16265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_splitter_prompt=\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d70a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# LLM\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "\n",
    "\n",
    "# Prompt\n",
    "question=\"Does the solution have reporting related to system health? If so, please explain the reporting and the granularity.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(f\"\"\"\n",
    "\n",
    "\n",
    "Query: <{question}>\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# query=\"Load balancing customer IO capabilities across nodes in a cluster or across geographic areas within a region\"\n",
    "\n",
    "chain.ainvoke({\"question\":f\"{question}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89712431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"queries\": [\n",
      "    {\n",
      "      \"query\": \"What is the maximum read performance of the product in terms of IOPS (Input/Output Operations Per Second)?\"\n",
      "    },\n",
      "    {\n",
      "      \"query\": \"What is the maximum write performance of the product in terms of IOPS (Input/Output Operations Per Second)?\"\n",
      "    },\n",
      "    {\n",
      "      \"query\": \"What is the maximum number of concurrent connections that the product can sustain?\"\n",
      "    },\n",
      "    {\n",
      "      \"query\": \"What is the product's throughput in terms of megabytes per second (MB/s) or gigabytes per second (GB/s) for read and write operations?\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\",temperature=0,\n",
    "                 nvidia_api_key=\"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\")\n",
    "\n",
    "import re,json\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# nvidia_api_key=\n",
    "# if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "# #     assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
    "\n",
    "question=\"Product performance capabilities - e.g.  read or written per minute. How many connections can the product sustain\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=f\"\"\"You are a helpful assistant in breaking down the customers-provided queries from the NetApp Storage Grid Data Storage Management System.\n",
    "\n",
    "You will be provided with a query.You have to provide the output in json format\n",
    "\n",
    "Follow the instructions to provide the output.\n",
    "\n",
    "1. Understand the query .\n",
    "2. break it into multiple queries if required.\n",
    "3. Provide the queries as output. STRICTLY Provide the output in json format with only the queries.\n",
    "     \"\"\"),\n",
    "    HumanMessage(content=f\"Here is the question:{question}\"),\n",
    "]\n",
    "\n",
    "chain=llm | StrOutputParser()\n",
    "\n",
    "questions=chain.invoke(messages)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\",temperature=0,\n",
    "                 nvidia_api_key=\"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\")\n",
    "result = llm.invoke(f\"You are a json extractor. Extract the json data from the given string {questions}.OUTPUT should be STRICTLY in JSON Format. only the text within curly braces\")\n",
    "\n",
    "\n",
    "import re\n",
    "match = re.search(r'```\\s*({.*?})\\s*```',result.content, re.DOTALL)\n",
    "if match:\n",
    "    extracted_json = match.group(1)\n",
    "    print(extracted_json)\n",
    "\n",
    "queries=json.loads(extracted_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a6bee715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the maximum read performance of the product in terms of IOPS (Input/Output Operations Per Second)?',\n",
       " 'What is the maximum write performance of the product in terms of IOPS (Input/Output Operations Per Second)?',\n",
       " 'What is the maximum number of concurrent connections that the product can sustain?',\n",
       " \"What is the product's throughput in terms of megabytes per second (MB/s) or gigabytes per second (GB/s) for read and write operations?\"]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries\n",
    "query_list = [item['query'] for item in queries['queries']]\n",
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "015a0cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the maximum read performance of the product in terms of IOPS (Input/Output Operations Per Second)?',\n",
       " 'What is the maximum write performance of the product in terms of IOPS (Input/Output Operations Per Second)?',\n",
       " 'What is the maximum number of concurrent connections that the product can sustain?',\n",
       " \"What is the product's throughput in terms of megabytes per second (MB/s) or gigabytes per second (GB/s) for read and write operations?\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c829552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
